---
title: "HW3-Part 2-Classification models"
author: "Brockington"
date: November 9,2025
format: 
  html:
    embed-resources: true
---

## Step 1 - Familiarize yourself with the data and the assignment

Save this file as a new Quarto Markdown document and name it something that
includes your last name in the filename. Save it into the
same folder as this file. Create a new R Studio Project based on this folder.

This assignment will focus on building simple classification models for
predicting fetal health based on a number of clinical measurements known
as *cardiotocographic data*. 

The following introductory information was taken from the main
[Kaggle Dataset page](https://www.kaggle.com/andrewmvd/fetal-health-classification) for this data:


>    **Context**

> Reduction of child mortality is reflected in several of the United Nations' Sustainable Development Goals and is a key indicator of human progress. The UN expects that by 2030, countries end preventable deaths of newborns and children under 5 years of age, with all countries aiming to reduce underâ€‘5 mortality to at least as low as 25 per 1,000 live births.
> 
Parallel to notion of child mortality is of course maternal mortality, which accounts for 295 000 deaths during and following pregnancy and childbirth (as of 2017). The vast majority of these deaths (94%) occurred in low-resource settings, and most could have been prevented.
>
> In light of what was mentioned above, Cardiotocograms (CTGs) are a simple and cost accessible option to assess fetal health, allowing healthcare professionals to take action in order to prevent child and maternal mortality. The equipment itself works by sending ultrasound pulses and reading its response, thus shedding light on fetal heart rate (FHR), fetal movements, uterine contractions and more.
>
> **Data**
>
> This dataset contains 2126 records of features extracted from Cardiotocogram exams, which were then classified by three expert obstetritians into 3 classes:

> * Normal
> * Suspect
> * Pathological

The definitions of the columns are:

    * baseline value - FHR baseline (beats per minute)
    * accelerations - Number of accelerations per second
    * fetal_movement - Number of fetal movements per second
    * uterine_contractions - Number of uterine contractions per second
    * light_decelerations - Number of light decelerations per second
    * severe_decelerations - Number of severe decelerations per second
    * prolongued_decelerations - Number of prolonged decelerations per second
    * abnormal_short_term_variability - Percentage of time with abnormal short term variability
    * mean_value_of_short_term_variability - Mean value of short term variability
    * percentage_of_time_with_abnormal_long_term_variability - Percentage of time with abnormal long term variability
    * mean_value_of_long_term_variability - Mean value of long term variability
    * histogram_width - Width of FHR histogram
    * histogram_min - Minimum (low frequency) of FHR histogram
    * histogram_max - Maximum (high frequency) of FHR histogram
    * histogram_number_of_peaks - Number of histogram peaks
    * histogram_number_of_zeroes - Number of histogram zeros
    * histogram_mode - Histogram mode
    * histogram_mean - Histogram mean
    * histogram_median - Histogram median
    * histogram_variance - Histogram variance
    * histogram_tendency - Histogram tendency

You can learn much more about the study behind this dataset from the following
published paper:

* https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6822315/

While in the original study and dataset, there were three classes for the
target variable (Normal, Suspect, Pathological), we will convert this to
a binary classification problem where 1 will be Suspect or Pathological and
0 will be Normal. Multi-class problems are a bit beyond the scope of this
introduction to classification problems.

```{r libraries}
library(dplyr)   # Group by analysis and other SQLish things.
library(ggplot2) # Plotting, of course
library(tidyr)   # Data reshaping
library(tidymodels)   # Many aspects of predictive modeling
library(corrplot)  # Correlation plots
library(skimr)       # An automated EDA tool (you saw this in a previous assignment)
library(rpart)
library(rpart.plot)
library(RColorBrewer)
library(vip)
```

## Step 2 - Load data

You'll notice that there is a subfolder named **data**.
Inside of it you'll find the data file for this assignment:

- **fetal_health.csv**


### Load the data

```{r load_data}
fetal_health <- read.csv("./data/fetal_health.csv")
```

Our target variable will be based on the `fetal_health` variable. Let's check the current values.

```{r}
table(fetal_health$fetal_health)
```

Now we will recode the 1's as a 0 and the 2's and 3's as a 1. I'm going to do this in a new column
so that we can check and make sure we've got it right.

```{r}
fetal_health$b_fetal_health <- fetal_health$fetal_health
fetal_health$b_fetal_health[fetal_health$b_fetal_health == 1] <- 0
fetal_health$b_fetal_health[fetal_health$b_fetal_health >= 2] <- 1

table(fetal_health$b_fetal_health, fetal_health$fetal_health)
```

Looks good, let's drop the original `fetal_health` column and convert the b_fetal_health` column
to a factor. I'm explicitly setting the order of the factor levels so that "0" is the first level just
so there's no confusion.

```{r}
fetal_health <- fetal_health  |>  
  select (!fetal_health) 

fetal_health$b_fetal_health <- factor(fetal_health$b_fetal_health, levels=c("0", "1"))
```

Find the number of patients and the percentage of patients for the two fetal health levels - 1 and 2. You'll
see that there are about 78% of the patients with a normal fetal health assessment (i.e. `b_fetal_health` = 0)

```{r target_prop_check}
prop.table(table(fetal_health$b_fetal_health))
```

Use `str`, `summary`, and `skim` to get a sense of the data. Our response variable, the thing we will be trying to predict is `b_fetal_health`. 

```{r firstlook}
str(fetal_health)
```



```{r}
summary(fetal_health)
```

```{r}
skim(fetal_health)
```

## Step 3 - Data partitioning

Ok, it's time to do an initial split of our data into training and test datasets.

For data partitioning, we will use the [rsample](https://rsample.tidymodels.org/) package. Let's allocate 80% of the records to our training set and 20% to the test set. Make sure you specify the `strata = ` argument. After partitioning, confirm that the `b_fetal_health` proportions of 1's and 0's are as expected. Discuss.

```{r partitioning_soln}
set.seed(983) # Do NOT change this value
pct_train <- 0.8

# Create a "split object" based on an 80/20, stratified split.
fetal_health_split <- rsample::initial_split(fetal_health,
                                        prop = pct_train,
                                        strata = b_fetal_health)

# Create train and test dataframes based on the split
fetal_health_train <- rsample::training(fetal_health_split)
fetal_health_test <- rsample::testing(fetal_health_split)
```


```{r}
prop.table(table(fetal_health$b_fetal_health))
prop.table(table(fetal_health_test$b_fetal_health))
prop.table(table(fetal_health_train$b_fetal_health))
```

> I believe that the data was successfully partitioned because the results are very close and there does not appear to be an imballance between them. 

## Step 4 - EDA

Do some EDA to try to uncover some relationships that may end up being
useful in building a predictive model for `b_fetal_health`. You can only use the training data for this. You learned
things in HW2 which should be useful here. In particular, you should at least do:

- a correlation plot
- faceted box or violin plots for all of the numeric variables with the target variable as the grouping variable,

I'll give you a little help for each of these.

### Correlation plot

> We'll start with a correlation plot. The `tl.cex` argument helps us display the 
plot by controlling the size of the text label. It's behavior is a little unpredictable but you can play around with various values. 

```{r corrplot}
corrplot(
    method = "square", 
    type = "full", 
    tl.cex = 0.55,
    cor(fetal_health_train[,1:21]))
  
```

> Based on this I am thinking that selcting one of the Histogram mean, median or mode variables would be the best selection as they have the strongest relationships
The next would probably be Histogram Variance or mean value of short term variablility as those have stronger positive and negative correlations across the board. 

Now make a bunch of boxplots (or violin) that are grouped by the target variable. There are a few ways we can do this.
 
Instead of creating separate plots for each variable, we could create
a faceted plot, where we facet by the variable. To do this, we need to reshape
our data from wide to long. We can do this using `tidyr::pivot_longer()`.

```{r reshape_longer}
 fetal_health_long <- fetal_health_train |> 
 # select(where(is.numeric)) %>%
   pivot_longer(
      cols = 1:21,
      names_to = "variable",
      values_to = "value"
   )
```



Now we can create the faceted plot using the long data.

```{r faceted_var_plot, fig.width=12, fig.height=16}


 fetal_health_long %>% 
   ggplot() + 
  geom_boxplot(aes(x=b_fetal_health, y=value, color=b_fetal_health)) +
   facet_wrap(~ variable, scales = "free", ncol=4) +
   ggtitle("Box plots by predictor") +
   xlab("Fetal health (0=normal)" )
```

> Looking at the box plots makes me lean away from the previous assumption of the histogram based variables as it appears the data is too similar and would not be good predictors.  I am thinking that the abnormal short term variability, prolongued decelerations, percenage of time with abnormal long term variability, and maybe uterine contractions would be good predictors.

## Step 5 - Building and evaluation of predictive classification models

Now that you know a little more about the data, it's time to start building a
few classification models for `b_fetal_health`. We will use:

- logistic regression
- a simple decision tree
- a random forest


We will start out using overall prediction accuracy
as our metric but we might want to consider other metrics.

**QUESTION** Why might overall prediction accuracy not be the most appropriate metric to consider? What other
metrics might be important and why?

> I think the overall prediction is a poor choice because of the large imbalance between normal and abnormal cases, meaning we do not have that high of accuracy with the overal prediction.
This is important since we are dealing with medical situations and incorrectly predicting a case is normal when it is actually abnormal can be life threatining.  So we want to be sure we have the highest accuracy possible when making predictions to reduce false predictions.

### The null model

A very simple model would be to simply predict that `b_fetal_health` is equal to 0. On
the training data we saw that we'd be ~78% accurate.

**QUESTION** What is the sensitivity and specificity of the null model?


> I am not sure on this one but I assume some mathing is involved. But I think that if we are placing the b_fetal_health at 0 that would mean we would have low sensitivity since we only have the one of two possible values so we are incorrectly gussing the other 22 percent of the abnormal cases.  However the specificity would be 1? since we would always predict 0 meaning we are always right?   Maybe I am not remembering this stuff correclty though, it's a bit jumbly in my head.

So, as we begin fitting more complicated models, remember that we need to
outperform the null model to make it worth it to use more complicated models.


### Model 1 -  logistic regression model

For logistic regression, you must use tidymodels and you should see our class notes on logistic regression, in particular, the section titled "Logistic regression with tidymodels". For this modeling technique, you must:

* fit at least two different logistic regression models, each of which should have at least five variables. For your first model, use variables that you think might have predictive value based on the EDA you did. For the second model, use all of the variables.
* use 5-fold cross-validation as our resampling scheme for model fitting,
* just take the default threshold value of 0.5,
* assess the model's performance on the training data (accuracy, sensitivity, specificity, confusion matrix),
* use the model to make predictions on the test data and assess the model's performance on the test data,
* discuss the results.

**HACKER EXTRA CREDIT** Create ROC curves and compute AUC for your logistic regression models. Discuss your interpretation of these plots.

Start by creating a classification model object using the appropriate `engine = ` argument for logistic regression.

```{r glm_spec}
glm_spec <- logistic_reg(mode = "classification", engine = "glm")
```


Create a recipe for your first model which specifies the formula for your logistic regression model.

```{r recipe_1}
 recipe_1 <-
   recipe(b_fetal_health ~  abnormal_short_term_variability + 
            prolongued_decelerations + 
            percentage_of_time_with_abnormal_long_term_variability + 
            uterine_contractions +
            histogram_mean,
            data = fetal_health_train)

```

Now create a workflow object and the recipe and model to it.

```{r}
 wf_1 <- workflow() %>% 
   add_recipe(recipe_1) %>% 
   add_model(glm_spec)
```

Now we'll create a folds object to use for k-crossfold validation.

```{r}
set.seed(259)
fetal_health_folds <- vfold_cv(fetal_health_train, v = 5)
ctrl_preds <- control_resamples(save_pred = TRUE, save_workflow = TRUE)
```

Now you should fit your logistic regression model using folds object we just created.

```{r}
 fit_results_logreg1 <- 
   wf_1 %>% 
   fit_resamples(resamples = fetal_health_folds, control = ctrl_preds)
```

Now we can collect the metrics.

```{r}
collect_metrics(fit_results_logreg1)
```

Let's refit on entire training data set and make predictions on the test data. Recall from the notes that we use the `last_fit` function to do this. Then we can use the `yardstick` package to compute accuracy, sensitivity and specificity. 

```{r}
last_fit_logreg1 <- last_fit(wf_1, fetal_health_split)

# These are prediction on the test data

final_predicted_classes_1 <- collect_predictions(last_fit_logreg1)

# Accuracy
 acc_test_logreg1 <- yardstick::accuracy(final_predicted_classes_1,
                 estimate = .pred_class,
                 truth = b_fetal_health, event_level = "second")

# Do similar things for sensitivity and specificity.
 sens_test_logreg1 <-  yardstick::sens(final_predicted_classes_1,
                 estimate = .pred_class,
                 truth = b_fetal_health, event_level = "second")

 spec_test_logreg1 <-  yardstick::spec(final_predicted_classes_1,
                 estimate = .pred_class,
                 truth = b_fetal_health, event_level = "second")

# Combine the results into a single dataframe

 stats_test_logreg1 <- bind_rows(acc_test_logreg1,
                                  sens_test_logreg1,
                                  spec_test_logreg1) |>
   mutate(data = 'test',
          model = 'logreg1')

# Confusion matrix
 yardstick::conf_mat(final_predicted_classes_1,
                 estimate = .pred_class,
                 truth = b_fetal_health)

 stats_test_logreg1


```

> The first model resulted in 27 false negatives It also has decent accuracy and specificiy however the sensitivity is a bit lower. Overall better than Null obviously, but it still had 27 false negatives which is not really great in the medical world.

Let's fit a second model using all of the variables.

```{r recipe_2}
recipe_2 <-
     recipe(b_fetal_health ~ .,
            data = fetal_health_train)
   
# Create your workflow object and add the recipe and model
wf_2 <- workflow() %>% 
   add_recipe(recipe_2) %>% 
   add_model(glm_spec)
```

Just like you did above, fit the model and assess its performance. Compare the results to the first logistic regression model. Use as many code chunks as needed.

```{r logreg2_fit_results}
 fit_results_logreg2 <- 
   wf_2 %>% 
   fit_resamples(resamples = fetal_health_folds, control = ctrl_preds)
```


```{r}
collect_metrics(fit_results_logreg2)
```

```{r}
last_fit_logreg2 <- last_fit(wf_2, fetal_health_split)

# These are prediction on the test data

final_predicted_classes_2 <- collect_predictions(last_fit_logreg2)

# Accuracy
 acc_test_logreg2 <- yardstick::accuracy(final_predicted_classes_2,
                 estimate = .pred_class,
                 truth = b_fetal_health, event_level = "second")

# Do similar things for sensitivity and specificity.
 sens_test_logreg2 <-  yardstick::sens(final_predicted_classes_2,
                 estimate = .pred_class,
                 truth = b_fetal_health, event_level = "second")

 spec_test_logreg2 <-  yardstick::spec(final_predicted_classes_2,
                 estimate = .pred_class,
                 truth = b_fetal_health, event_level = "second")

# Combine the results into a single dataframe

 stats_test_logreg2 <- bind_rows(acc_test_logreg2,
                                  sens_test_logreg2,
                                  spec_test_logreg2) |>
   mutate(data = 'test',
          model = 'logreg2')

# Confusion matrix
 yardstick::conf_mat(final_predicted_classes_2,
                 estimate = .pred_class,
                 truth = b_fetal_health)

 stats_test_logreg2
```



> This model had only 14 false negatives so was much better than the first model.   Both accuracy and sensitivity improved, with sensitivity impoving quite a bit.  Specificity is a little lower but pretty much a neglegible difference.  Overall this is a better model so far but it would be better to get fewer false negatives. 


### Model 2 - a simple decision tree

For this model, we will **NOT** use a resampling scheme (i.e. no k-fold cross-validation). We will just use our one simple train-test split. We'll use `rpart` for the engine, just as we did in the notes.

Create your model specification.

```{r tree_spec}
 tree_spec <- decision_tree() %>%
   set_engine("rpart") %>% 
   set_mode("classification")

```

Let's fit a tree to the training data using just a few variables so that we
can easily see what's going on.

```{r tree1_fit}
tree1_fit <- fit(tree_spec, b_fetal_health ~  
           accelerations + 
           baseline.value +
           histogram_median + +
           uterine_contractions, data = fetal_health_train)
```

Let's see what the fitted object looks like.

```{r}
tree1_fit
```

The `rpart.plot()` function lets us see the actual (upside down) tree. Since
this function only works with `rpart` objects, we need to extract it from our
`fit` object. 


```{r tree1_plot}
tree1_fit %>%
  extract_fit_engine() %>%
  rpart.plot(tweak = 1.2)
```

**QUESTION** Explain each of the values in the node in the third row from the top in the middle (light blue with values 0, 0.31 and 26%) What conditions need to be true for a case to end up in that node?

> For the node itself the number 0 is the prediction for B_fetal_health of 0 meaning normal.  The .31 is the probability of the cases in that node being predicted as 0.  the 26% is the percentage of the total data results that went to that node.
To get there one would have to have a Accelerations less than or equal to 0.0005  and then uterine contractions greater than or equal to .0025


Now use `augment()` function along with `yardstick::sens()`, `yardstick::spec()`, and `yardstick::conf_mat()` to compute sensitivity, specificity, and the confusion matrix on the **training data**. In other words, we are just assessing how well the tree fits the data. 

Notice in the code skeleton that I'm explicitly specifying the **yardstick** package and a function from it. I'm doing that because the `spec` function name clashes with another library we have loaded - you can see this by doing a `help(spec)`. Also notice that we need `event_level = "second"` since "0" comes before "1" in the factor levels for `b_fetal_health` and it's the "1"'s that are the thing we are trying to detect (abnormal results). We need `event_level = "second" for accuracy, sensitivity and specificity calculations (we don't need it for the confusion matrix).

```{r tree1_metrics_train}
# accuracy
acc_train_tree <- augment(tree1_fit, new_data = fetal_health_train) %>%
   yardstick::accuracy(truth = b_fetal_health, 
                       estimate = .pred_class,
                       event_level = "second")

# Sensitivity
sens_train_tree <- augment(tree1_fit, new_data = fetal_health_train) %>%
   yardstick::sens(truth = b_fetal_health, 
                       estimate = .pred_class,
                       event_level = "second")
  
# Specificity
spec_train_tree <- augment(tree1_fit, new_data = fetal_health_train) %>%
   yardstick::spec(truth = b_fetal_health, 
                       estimate = .pred_class,
                       event_level = "second")

 
stats_train_tree1 <- bind_rows(acc_train_tree, sens_train_tree, spec_train_tree) %>% 
  mutate(data = 'train',
         model = 'tree1')
# Confusion matrix
 augment(tree1_fit, new_data = fetal_health_train) %>%
   yardstick::conf_mat(truth = b_fetal_health, 
                       estimate = .pred_class,
                       event_level = "second")
 
 stats_train_tree1
```


**QUESTION** What is the predicted class and the probabilities of 0 and 1 for the very first row in `fetal_health_train`? **HINT** There is nothing to compute - you just need to look at the appropriate tibble and you've already used this tibble above.

```{r explore_first_row}
augment(tree1_fit, new_data = fetal_health_train) %>%
select(b_fetal_health, .pred_class, .pred_0, .pred_1) %>%
  head(1)
```

Now, make predictions for the test data for this first tree and compute sensitivity, specificity and the confusion matrix. Remember (see notes) that you can use the `augment()` function to compute predicted values for the test data.


```{r tree1_metrics_test}
# Accuracy
 acc_test_tree <- augment(tree1_fit, new_data = fetal_health_test) %>%
   yardstick::accuracy(truth = b_fetal_health, 
                       estimate = .pred_class,
                       event_level = "second")

# Sensitivity
 sens_test_tree <- augment(tree1_fit, new_data = fetal_health_test) %>%
   yardstick::sens(truth = b_fetal_health, 
                       estimate = .pred_class,
                       event_level = "second")

# Specificity
 spec_test_tree <- augment(tree1_fit, new_data = fetal_health_test) %>%
   yardstick::spec(truth = b_fetal_health, 
                       estimate = .pred_class,
                       event_level = "second")
 
 
 
stats_test_tree1 <- bind_rows(acc_test_tree, sens_test_tree, spec_test_tree) %>% 
  mutate(data = 'test',
         model = 'tree1')

# Confusion matrix
 augment(tree1_fit, new_data = fetal_health_test) %>%
   yardstick::conf_mat(truth = b_fetal_health, estimate = .pred_class)

stats_test_tree1

```

**QUESTION** Compare the results to what you got on the training data. Did the metrics get better or worse? Is this expected? Why? Is there evidence of overfitting? 

Here's some code that could be useful in organizing and comparing the results.

```{r tree_compare_soln}
stats_tree1 <- bind_rows(stats_train_tree1, stats_test_tree1)
 stats_tree1 |> 
   select(.metric, .estimate, data) |> 
   pivot_wider(names_from = data, values_from = .estimate)
```


> It looks like everything got worse but I think thats ok since training data is suppsed to be better and introducing test data expects a drop.  I still don't quite understand overfitting but I think the fact that the numbers dropped a bit could lead to overfitting but it doesn't seem like that big of a drop. 

Now, we'll clean up the workspace a bit before trying the random forest.

```{r}
rm(tree1_fit)
```

### Model 3 - Random Forest

Instead of using a simple decision tree, use a random forest. Obviously, you should now use all the variables that you think might be useful for predicting `b_fetal_health`.

* fit the model on the training data,
* assess the model's performance on the training data using the `augment()` function like we did for the decision tree,
* use the model to make predictions on the test data and assess the model's performance on the test data using the `augment()` function,
* create an importance plot to get a sense of the relative importance of the different variables,
* discuss the results

In your discussion of the results you should talk about things like:

* how accurate is the model in predicting on the test data?
* is their evidence of overfitting?
* how does the model do in terms of other metrics like sensitivity and specificity?
* how does the performance of the random forest compare to the simple decision tree?

```{r rf_spec}
 rf_spec <- rand_forest() %>%
   set_engine("randomForest", importance = TRUE, na.action = na.omit) %>%
   set_mode("classification")
```

Fit the model

```{r rf_fit}
 rf_fit <- fit(rf_spec, b_fetal_health ~ accelerations +
                 histogram_median +
                 baseline.value +
                 uterine_contractions +
                 abnormal_short_term_variability +
                 prolongued_decelerations , data = fetal_health_train)

```


Compute accuracy, sensitivity and specificity for the fit.

```{r rf_fit_metrics}
acc_train_rf <- augment(rf_fit, new_data = fetal_health_train) %>%
  accuracy(truth = b_fetal_health, estimate = .pred_class)

sens_train_rf <- augment(rf_fit, new_data = fetal_health_train) %>%
  sens(truth = b_fetal_health, estimate = .pred_class)

spec_train_rf <- augment(rf_fit, new_data = fetal_health_train) %>%
  spec(truth = b_fetal_health, estimate = .pred_class)

stats_train_rf <- bind_rows(acc_train_rf, sens_train_rf, spec_train_rf) %>% 
  mutate(data = 'train_rf',
         model = 'rf_train')
```

Let's look at an importance plot to get a sense of which variables seem to be influential.


```{r}
 vip(rf_fit)
```
Now let's compute accuracy, sensitivity and specificity on the test data.

```{r rf_test_metrics_test}
 rf_fit_test <- fit(rf_spec, b_fetal_health ~ accelerations +
                 histogram_median +
                 baseline.value +
                 uterine_contractions +
                 abnormal_short_term_variability +
                 prolongued_decelerations , data = fetal_health_test)
```


Compute accuracy, sensitivity and specificity for the fit.

```{r rf_fit_metrics_test}
acc_test_rf <- augment(rf_fit_test, new_data = fetal_health_train) %>%
  accuracy(truth = b_fetal_health, estimate = .pred_class)

sens_test_rf <- augment(rf_fit_test, new_data = fetal_health_train) %>%
  sens(truth = b_fetal_health, estimate = .pred_class)

spec_test_rf <- augment(rf_fit_test, new_data = fetal_health_train) %>%
  spec(truth = b_fetal_health, estimate = .pred_class)


stats_test_rf <- bind_rows(acc_test_rf, sens_test_rf, spec_test_rf) %>% 
  mutate(data = 'test_rf',
         model = 'rf_test')
```

Let's look at an importance plot to get a sense of which variables seem to be influential.


```{r}
 vip(rf_fit) +
ggplot2::labs(title = "train data")
vip(rf_fit_test) +
ggplot2::labs(title = "test data")

stats_rf <- bind_rows(stats_train_rf, stats_test_rf)
 stats_rf |> 
   select(.metric, .estimate, data) |> 
   pivot_wider(names_from = data, values_from = .estimate)
```
How do test and train performance compare?

> Overall these models are far more accurate at their predictions however I think overfit may be an issue due to the big drop in Specificity from the train to test data.  But I would say this fared better than the previous models but could do better with different or more variables.  I could also see running more tests with different variables to use the importantce plot to try and find the best variables to fit with.

```{r}
rm(rf_fit)
```

## Final model comparisons

```{r overall_summary}
stats_tree1 <- bind_rows(stats_train_tree1, stats_train_rf, stats_test_tree1, stats_test_rf)
 stats_tree1 |> 
   select(.metric, .estimate, data) |> 
   pivot_wider(names_from = data, values_from = .estimate)
```


So, if you had to recommend a model to consider using in practice, which of the models (if any) would you recommend and why?

> It seems very clear to me that based on these numbers the random forest models are much better at predicting the fetal health.  The random forest would need some more work to ensure it has a better specificit.  I think that would be done through finding better variables or potentially adding a few more to the model.  BUt overall we would want to get the model to better at ensuring it has a high specificity to be sure we are minimizing false positives as much as possible.